{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Prepare Apache Spark\n",
    "\n",
    "## Install Spark\n",
    "\n",
    "You can install Apache Spark from this [link](https://spark.apache.org/downloads.html).\n",
    "We will download the latest stable version v2.4.1 pre-built for Apache Hadoop 2.7 or later from\n",
    "this [link](https://www.apache.org/dyn/closer.lua/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz).\n",
    "\n",
    "\n",
    "```\n",
    "tar -xzvf spark-2.4.1-bin-hadoop2.7.tgz\n",
    "cd spark-2.4.1-bin-hadoop2.7\n",
    "\n",
    "export SPARK_HOME='<SPARK_INSTALLATION_FOLDER>/spark-2.4.1-bin-hadoop2.7'\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "You can now run interactive `spark-shell` or `pyspark` sessions.\n",
    "\n",
    "You can add `SPARK_HOME` and `PATH` parmanently to your profile (`~/.bashrc`) on ubuntu.\n",
    "\n",
    "## Start SparkUI\n",
    "\n",
    "Spark History server provides a web user interface to inspect and analyse completed and running Spark Jobs.\n",
    "\n",
    "Before running the Spark History server, we must ensure that Spark Jobs are configured to write their event logs.\n",
    "\n",
    "```\n",
    "cd $SPARK_HOME\n",
    "cp conf/spark-defaults.conf.template conf/spark-defaults.conf\n",
    "vi conf/spark-defaults.conf\n",
    "```\n",
    "\n",
    "In `spark-defaults.conf` configuration file, uncomment:\n",
    "\n",
    "```\n",
    "spark.eventLog.enabled           true\n",
    "\n",
    "```\n",
    "\n",
    "By default, Spark events are logged to `/tmp/spark-events`, you can override this by setting `spark.eventLog.dir`\n",
    "to a local folder (fine for standalone deployment) or to an HDFS folder.\n",
    "\n",
    "Now start the Spark History server with:\n",
    "\n",
    "```\n",
    "$SPARK_HOME/sbin/start-history-server.sh\n",
    "```\n",
    "\n",
    "Open your browser and goto `localhost:18080`.\n",
    "\n",
    "## Install and prepare Jupyter\n",
    "\n",
    "In this tutorial, we will use [Jupyter](https://jupyter.org/) notebook as our\n",
    "interactive development environment.\n",
    "\n",
    "You can follow the instructions in the [install](https://jupyter.org/install.html)\n",
    "guide to install Jupyter on your machine.\n",
    "\n",
    "In order to get `pyspark` to work in Jupyter notebook, we need to set some environment varibales:\n",
    "\n",
    "```\n",
    "export PYSPARK_DRIVER_PYTHON=\"jupyter\"\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
    "```\n",
    "\n",
    "You can add these environment variables permanently to your profile.\n",
    "\n",
    "Now running `pyspark` will start jupyter.\n",
    "\n",
    "```\n",
    "pyspark\n",
    "```\n",
    "\n",
    "You can check this [article](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Apache Spark\n",
    "\n",
    "In this tutorial, we will use Apache Spark for `WebLog` analysis.\n",
    "We will be using the `dataframe` API as it is the most common Structured API in\n",
    "Spark. A dataframe represents a table of data with rows and columns. A dataframe\n",
    "always has a `schema` defining the column data types and some additional `metadata`\n",
    "like `nullable` indicating if the column accepts `nulls`.\n",
    "\n",
    "## Load the web logs\n",
    "\n",
    "The `Apache server log` is a text based format with custom structure (similar to\n",
    "tabular format). It can't be directly loaded into Apache Spark.\n",
    "We need to parse it line by line.\n",
    "\n",
    "This can be done by reading a text file into an `RDD`, mapping every line into a\n",
    "`pyspark.sql.Row` and transform the `RDD` into a Spark `dataframe`.\n",
    "\n",
    "An Apache server log line can be parsed using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expression\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# regular expression to parse an Apache Server log line\n",
    "# It is constructed as follows\n",
    "# IP_Address client_id user_id date_time method endpoint protocol response_code content_size\n",
    "LOG_PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+)'\n",
    "\n",
    "# Returns a dataframe Row including named arguments. The fields in a Row are\n",
    "# sorted by name. The fields of the returned row correspond to the Apache Server\n",
    "# Log fields.\n",
    "def log_parser(line):\n",
    "  # Now match the given input line with the regular expression\n",
    "  match = re.search(LOG_PATTERN, line)\n",
    "\n",
    "  # If there is no match, then the input line is invalid: report an error\n",
    "  if match is None:\n",
    "    raise Error(\"Invalid input line: %s\" % line)\n",
    "\n",
    "  # return a pyspark.sql.Row\n",
    "  return Row(\n",
    "    ip_address    = match.group(1), # IP address of the client (IPv4 or IPv6)\n",
    "    client_id     = match.group(2), # Clientid: mostly '-'. This info should never be used as it is unreliable.\n",
    "    user_id       = match.group(3), # The userid as defined in HTTP authentication\n",
    "                                    # If the endpoint is not password protected, it will be '-'\n",
    "    date_time     = match.group(4), # The time the server finished processing the request\n",
    "                                    # date_time format: day/month/year:hour:minute:second zone\n",
    "    method        = match.group(5), # The HTTP method\n",
    "    endpoint      = match.group(6), # The requested endpoint\n",
    "    protocol      = match.group(7), # The protocol in use, usually HTTP/version\n",
    "    response_code = int(match.group(8)), # one of HTTP's response codes (< 599).\n",
    "    content_size  = int(match.group(9)) # content size as reported in the HTTP\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the Weblog data. Don't forget to set `logfile` to the full path\n",
    "to weblog data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to weblog data\n",
    "# logfile = 'REPLACE_WITH_PATH_TO_WEBLOG.LOG'\n",
    "logfile = '/home/ubuntu/Desktop/dev/datafiles/weblog.log'\n",
    "\n",
    "# reads the Weblog as a text file and returns it as RDD of lines\n",
    "# applies the log_parser function to every line in the RDD\n",
    "# Transforms the RDD into a Spark Dataframe\n",
    "input_df = sc.textFile(logfile).map(log_parser).toDF() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will apply multiple transformations to the weblog dataframe.\n",
    "In such case, it is beneficial to `cache` the dataframe in order to accelerate\n",
    "future access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = input_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Weblog data\n",
    "\n",
    "Now that the log in cached in a dataframe, let's explore some of its content:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------------------+--------------------+--------------------+------+--------+-------------+-------+\n",
      "|client_id|content_size|           date_time|            endpoint|          ip_address|method|protocol|response_code|user_id|\n",
      "+---------+------------+--------------------+--------------------+--------------------+------+--------+-------------+-------+\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/Ra...|  ::ffff:54.243.49.1|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/Ra...|  ::ffff:54.243.49.1|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/Aa...| ::ffff:84.85.189.28|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/Kl...| ::ffff:86.103.139.9|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/Kl...| ::ffff:86.103.139.9|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/Sa...| ::ffff:84.85.189.28|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/bl...| ::ffff:87.174.32.47|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/write/Ra...|  ::ffff:54.243.49.1|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|      -|\n",
      "|        -|         307|01/Mar/2018:23:07...|/v1/data/write/Ch...|::ffff:62.240.166...|  POST|HTTP/1.1|          200|      -|\n",
      "+---------+------------+--------------------+--------------------+--------------------+------+--------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() # prints up to 20 rows (default) of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter provides a better way to explore `pandas` dataframes as tables. \n",
    "> This will require converting the Spark Dataframe into Pandas Dataframe. This operation should only be executed on a subset of the dataframe.\n",
    "\n",
    "This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>content_size</th>\n",
       "      <th>date_time</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>ip_address</th>\n",
       "      <th>method</th>\n",
       "      <th>protocol</th>\n",
       "      <th>response_code</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:31 +0000</td>\n",
       "      <td>/v1/data/write/Ras_Beirut/statusRas_Beirut4</td>\n",
       "      <td>::ffff:54.243.49.1</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:31 +0000</td>\n",
       "      <td>/v1/data/write/Ras_Beirut/statusRas_Beirut1</td>\n",
       "      <td>::ffff:54.243.49.1</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:31 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:31 +0000</td>\n",
       "      <td>/v1/data/write/Aanbouw/CVSturing</td>\n",
       "      <td>::ffff:84.85.189.28</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:32 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:33 +0000</td>\n",
       "      <td>/v1/data/write/Klima/Temperatur</td>\n",
       "      <td>::ffff:86.103.139.9</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:33 +0000</td>\n",
       "      <td>/v1/data/write/Klima/Luftfeuchtigkeit</td>\n",
       "      <td>::ffff:86.103.139.9</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:33 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:34 +0000</td>\n",
       "      <td>/v1/data/write/bbt_raspi/cpu</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:34 +0000</td>\n",
       "      <td>/v1/data/write/bbt_raspi/memory</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:34 +0000</td>\n",
       "      <td>/v1/data/write/bbt_raspi/disk</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:34 +0000</td>\n",
       "      <td>/v1/data/write/bbt_raspi/eth0</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:34 +0000</td>\n",
       "      <td>/v1/data/write/Sacristie/HuidigeTemperatuur</td>\n",
       "      <td>::ffff:84.85.189.28</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:34 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:35 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:36 +0000</td>\n",
       "      <td>/v1/data/write/blume_2/Bodenfeuchtigkeit</td>\n",
       "      <td>::ffff:87.174.32.47</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:36 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:36 +0000</td>\n",
       "      <td>/v1/data/write/Ras_Beirut/statusRas_Beirut5</td>\n",
       "      <td>::ffff:54.243.49.1</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:37 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-</td>\n",
       "      <td>307</td>\n",
       "      <td>01/Mar/2018:23:07:37 +0000</td>\n",
       "      <td>/v1/data/write/Chvalenice?token=1486453156996_...</td>\n",
       "      <td>::ffff:62.240.166.118</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:38 +0000</td>\n",
       "      <td>/v1/data/write/Aanbouw/CVVermogen</td>\n",
       "      <td>::ffff:84.85.189.28</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:38 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:39 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:40 +0000</td>\n",
       "      <td>/v1/data/write/EricsBeer/Temp1</td>\n",
       "      <td>::ffff:146.198.224.141</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:40 +0000</td>\n",
       "      <td>/v1/data/write/RaspberryPiWeatherStation/AirPr...</td>\n",
       "      <td>::ffff:60.240.146.239</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:40 +0000</td>\n",
       "      <td>/v1/data/write/EricsBeer/Temp2</td>\n",
       "      <td>::ffff:146.198.224.141</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:40 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:40 +0000</td>\n",
       "      <td>/v1/data/write/ISS/pos</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:40 +0000</td>\n",
       "      <td>/v1/data/write/RaspberryPiWeatherStation/Tempe...</td>\n",
       "      <td>::ffff:60.240.146.239</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:07:41 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:29 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:29 +0000</td>\n",
       "      <td>/v1/data/write/Hamra/statusHamra4</td>\n",
       "      <td>::ffff:54.243.49.1</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:30 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:31 +0000</td>\n",
       "      <td>/v1/data/publish/rpi_zero</td>\n",
       "      <td>::ffff:69.47.237.44</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:31 +0000</td>\n",
       "      <td>/v1/data/write/Ras_Beirut/statusRas_Beirut3</td>\n",
       "      <td>::ffff:54.243.49.1</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:31 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:32 +0000</td>\n",
       "      <td>/v1/data/write/Klima/Temperatur</td>\n",
       "      <td>::ffff:86.103.139.9</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:32 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:32 +0000</td>\n",
       "      <td>/v1/data/write/Klima/Luftfeuchtigkeit</td>\n",
       "      <td>::ffff:86.103.139.9</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:33 +0000</td>\n",
       "      <td>/v1/data/write/Klima/Temperatur</td>\n",
       "      <td>::ffff:86.103.139.9</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:33 +0000</td>\n",
       "      <td>/v1/data/write/Klima/Luftfeuchtigkeit</td>\n",
       "      <td>::ffff:86.103.139.9</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:34 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:34 +0000</td>\n",
       "      <td>/v1/data/write/ISS/pos</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:34 +0000</td>\n",
       "      <td>/v1/data/write/bbt_raspi/cpu</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:34 +0000</td>\n",
       "      <td>/v1/data/write/bbt_raspi/memory</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:34 +0000</td>\n",
       "      <td>/v1/data/write/bbt_raspi/disk</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:34 +0000</td>\n",
       "      <td>/v1/data/write/bbt_raspi/eth0</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:35 +0000</td>\n",
       "      <td>/v1/data/write/Ras_Beirut/statusRas_Beirut1</td>\n",
       "      <td>::ffff:54.243.49.1</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:35 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:36 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>-</td>\n",
       "      <td>311</td>\n",
       "      <td>01/Mar/2018:23:12:36 +0000</td>\n",
       "      <td>/v1/data/write/Chvalenice?token=1486453156996_...</td>\n",
       "      <td>::ffff:62.240.166.118</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:37 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:38 +0000</td>\n",
       "      <td>/v1/data/write/Aanbouw/CVVermogen</td>\n",
       "      <td>::ffff:84.85.189.28</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:38 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:39 +0000</td>\n",
       "      <td>/v1/data/publish/ISS/position</td>\n",
       "      <td>::ffff:54.221.205.80</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:39 +0000</td>\n",
       "      <td>/v1/data/write/Klima/Temperatur</td>\n",
       "      <td>::ffff:86.103.139.9</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:39 +0000</td>\n",
       "      <td>/v1/data/write/Ras_Beirut/statusRas_Beirut4</td>\n",
       "      <td>::ffff:54.243.49.1</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:40 +0000</td>\n",
       "      <td>/v1/data/write/Klima/Temperatur</td>\n",
       "      <td>::ffff:86.103.139.9</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:40 +0000</td>\n",
       "      <td>/v1/data/write/Klima/Luftfeuchtigkeit</td>\n",
       "      <td>::ffff:86.103.139.9</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>01/Mar/2018:23:12:40 +0000</td>\n",
       "      <td>/v1/data/write/Klima/Luftfeuchtigkeit</td>\n",
       "      <td>::ffff:86.103.139.9</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    client_id  content_size                   date_time  \\\n",
       "0           -             4  01/Mar/2018:23:07:31 +0000   \n",
       "1           -             4  01/Mar/2018:23:07:31 +0000   \n",
       "2           -             4  01/Mar/2018:23:07:31 +0000   \n",
       "3           -             4  01/Mar/2018:23:07:31 +0000   \n",
       "4           -             4  01/Mar/2018:23:07:32 +0000   \n",
       "5           -             4  01/Mar/2018:23:07:33 +0000   \n",
       "6           -             4  01/Mar/2018:23:07:33 +0000   \n",
       "7           -             4  01/Mar/2018:23:07:33 +0000   \n",
       "8           -             4  01/Mar/2018:23:07:34 +0000   \n",
       "9           -             4  01/Mar/2018:23:07:34 +0000   \n",
       "10          -             4  01/Mar/2018:23:07:34 +0000   \n",
       "11          -             4  01/Mar/2018:23:07:34 +0000   \n",
       "12          -             4  01/Mar/2018:23:07:34 +0000   \n",
       "13          -             4  01/Mar/2018:23:07:34 +0000   \n",
       "14          -             4  01/Mar/2018:23:07:35 +0000   \n",
       "15          -             4  01/Mar/2018:23:07:36 +0000   \n",
       "16          -             4  01/Mar/2018:23:07:36 +0000   \n",
       "17          -             4  01/Mar/2018:23:07:36 +0000   \n",
       "18          -             4  01/Mar/2018:23:07:37 +0000   \n",
       "19          -           307  01/Mar/2018:23:07:37 +0000   \n",
       "20          -             4  01/Mar/2018:23:07:38 +0000   \n",
       "21          -             4  01/Mar/2018:23:07:38 +0000   \n",
       "22          -             4  01/Mar/2018:23:07:39 +0000   \n",
       "23          -             4  01/Mar/2018:23:07:40 +0000   \n",
       "24          -             4  01/Mar/2018:23:07:40 +0000   \n",
       "25          -             4  01/Mar/2018:23:07:40 +0000   \n",
       "26          -             4  01/Mar/2018:23:07:40 +0000   \n",
       "27          -             4  01/Mar/2018:23:07:40 +0000   \n",
       "28          -             4  01/Mar/2018:23:07:40 +0000   \n",
       "29          -             4  01/Mar/2018:23:07:41 +0000   \n",
       "..        ...           ...                         ...   \n",
       "970         -             4  01/Mar/2018:23:12:29 +0000   \n",
       "971         -             4  01/Mar/2018:23:12:29 +0000   \n",
       "972         -             4  01/Mar/2018:23:12:30 +0000   \n",
       "973         -             4  01/Mar/2018:23:12:31 +0000   \n",
       "974         -             4  01/Mar/2018:23:12:31 +0000   \n",
       "975         -             4  01/Mar/2018:23:12:31 +0000   \n",
       "976         -             4  01/Mar/2018:23:12:32 +0000   \n",
       "977         -             4  01/Mar/2018:23:12:32 +0000   \n",
       "978         -             4  01/Mar/2018:23:12:32 +0000   \n",
       "979         -             4  01/Mar/2018:23:12:33 +0000   \n",
       "980         -             4  01/Mar/2018:23:12:33 +0000   \n",
       "981         -             4  01/Mar/2018:23:12:34 +0000   \n",
       "982         -             4  01/Mar/2018:23:12:34 +0000   \n",
       "983         -             4  01/Mar/2018:23:12:34 +0000   \n",
       "984         -             4  01/Mar/2018:23:12:34 +0000   \n",
       "985         -             4  01/Mar/2018:23:12:34 +0000   \n",
       "986         -             4  01/Mar/2018:23:12:34 +0000   \n",
       "987         -             4  01/Mar/2018:23:12:35 +0000   \n",
       "988         -             4  01/Mar/2018:23:12:35 +0000   \n",
       "989         -             4  01/Mar/2018:23:12:36 +0000   \n",
       "990         -           311  01/Mar/2018:23:12:36 +0000   \n",
       "991         -             4  01/Mar/2018:23:12:37 +0000   \n",
       "992         -             4  01/Mar/2018:23:12:38 +0000   \n",
       "993         -             4  01/Mar/2018:23:12:38 +0000   \n",
       "994         -             4  01/Mar/2018:23:12:39 +0000   \n",
       "995         -             4  01/Mar/2018:23:12:39 +0000   \n",
       "996         -             4  01/Mar/2018:23:12:39 +0000   \n",
       "997         -             4  01/Mar/2018:23:12:40 +0000   \n",
       "998         -             4  01/Mar/2018:23:12:40 +0000   \n",
       "999         -             4  01/Mar/2018:23:12:40 +0000   \n",
       "\n",
       "                                              endpoint  \\\n",
       "0          /v1/data/write/Ras_Beirut/statusRas_Beirut4   \n",
       "1          /v1/data/write/Ras_Beirut/statusRas_Beirut1   \n",
       "2                        /v1/data/publish/ISS/position   \n",
       "3                     /v1/data/write/Aanbouw/CVSturing   \n",
       "4                        /v1/data/publish/ISS/position   \n",
       "5                      /v1/data/write/Klima/Temperatur   \n",
       "6                /v1/data/write/Klima/Luftfeuchtigkeit   \n",
       "7                        /v1/data/publish/ISS/position   \n",
       "8                         /v1/data/write/bbt_raspi/cpu   \n",
       "9                      /v1/data/write/bbt_raspi/memory   \n",
       "10                       /v1/data/write/bbt_raspi/disk   \n",
       "11                       /v1/data/write/bbt_raspi/eth0   \n",
       "12         /v1/data/write/Sacristie/HuidigeTemperatuur   \n",
       "13                       /v1/data/publish/ISS/position   \n",
       "14                       /v1/data/publish/ISS/position   \n",
       "15            /v1/data/write/blume_2/Bodenfeuchtigkeit   \n",
       "16                       /v1/data/publish/ISS/position   \n",
       "17         /v1/data/write/Ras_Beirut/statusRas_Beirut5   \n",
       "18                       /v1/data/publish/ISS/position   \n",
       "19   /v1/data/write/Chvalenice?token=1486453156996_...   \n",
       "20                   /v1/data/write/Aanbouw/CVVermogen   \n",
       "21                       /v1/data/publish/ISS/position   \n",
       "22                       /v1/data/publish/ISS/position   \n",
       "23                      /v1/data/write/EricsBeer/Temp1   \n",
       "24   /v1/data/write/RaspberryPiWeatherStation/AirPr...   \n",
       "25                      /v1/data/write/EricsBeer/Temp2   \n",
       "26                       /v1/data/publish/ISS/position   \n",
       "27                              /v1/data/write/ISS/pos   \n",
       "28   /v1/data/write/RaspberryPiWeatherStation/Tempe...   \n",
       "29                       /v1/data/publish/ISS/position   \n",
       "..                                                 ...   \n",
       "970                      /v1/data/publish/ISS/position   \n",
       "971                  /v1/data/write/Hamra/statusHamra4   \n",
       "972                      /v1/data/publish/ISS/position   \n",
       "973                          /v1/data/publish/rpi_zero   \n",
       "974        /v1/data/write/Ras_Beirut/statusRas_Beirut3   \n",
       "975                      /v1/data/publish/ISS/position   \n",
       "976                    /v1/data/write/Klima/Temperatur   \n",
       "977                      /v1/data/publish/ISS/position   \n",
       "978              /v1/data/write/Klima/Luftfeuchtigkeit   \n",
       "979                    /v1/data/write/Klima/Temperatur   \n",
       "980              /v1/data/write/Klima/Luftfeuchtigkeit   \n",
       "981                      /v1/data/publish/ISS/position   \n",
       "982                             /v1/data/write/ISS/pos   \n",
       "983                       /v1/data/write/bbt_raspi/cpu   \n",
       "984                    /v1/data/write/bbt_raspi/memory   \n",
       "985                      /v1/data/write/bbt_raspi/disk   \n",
       "986                      /v1/data/write/bbt_raspi/eth0   \n",
       "987        /v1/data/write/Ras_Beirut/statusRas_Beirut1   \n",
       "988                      /v1/data/publish/ISS/position   \n",
       "989                      /v1/data/publish/ISS/position   \n",
       "990  /v1/data/write/Chvalenice?token=1486453156996_...   \n",
       "991                      /v1/data/publish/ISS/position   \n",
       "992                  /v1/data/write/Aanbouw/CVVermogen   \n",
       "993                      /v1/data/publish/ISS/position   \n",
       "994                      /v1/data/publish/ISS/position   \n",
       "995                    /v1/data/write/Klima/Temperatur   \n",
       "996        /v1/data/write/Ras_Beirut/statusRas_Beirut4   \n",
       "997                    /v1/data/write/Klima/Temperatur   \n",
       "998              /v1/data/write/Klima/Luftfeuchtigkeit   \n",
       "999              /v1/data/write/Klima/Luftfeuchtigkeit   \n",
       "\n",
       "                 ip_address method  protocol  response_code user_id  \n",
       "0        ::ffff:54.243.49.1   POST  HTTP/1.1            200       -  \n",
       "1        ::ffff:54.243.49.1   POST  HTTP/1.1            200       -  \n",
       "2      ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "3       ::ffff:84.85.189.28   POST  HTTP/1.1            200       -  \n",
       "4      ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "5       ::ffff:86.103.139.9   POST  HTTP/1.1            200       -  \n",
       "6       ::ffff:86.103.139.9   POST  HTTP/1.1            200       -  \n",
       "7      ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "8      ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "9      ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "10     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "11     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "12      ::ffff:84.85.189.28   POST  HTTP/1.1            200       -  \n",
       "13     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "14     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "15      ::ffff:87.174.32.47   POST  HTTP/1.1            200       -  \n",
       "16     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "17       ::ffff:54.243.49.1   POST  HTTP/1.1            200       -  \n",
       "18     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "19    ::ffff:62.240.166.118   POST  HTTP/1.1            200       -  \n",
       "20      ::ffff:84.85.189.28   POST  HTTP/1.1            200       -  \n",
       "21     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "22     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "23   ::ffff:146.198.224.141   POST  HTTP/1.1            200       -  \n",
       "24    ::ffff:60.240.146.239   POST  HTTP/1.1            200       -  \n",
       "25   ::ffff:146.198.224.141   POST  HTTP/1.1            200       -  \n",
       "26     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "27     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "28    ::ffff:60.240.146.239   POST  HTTP/1.1            200       -  \n",
       "29     ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "..                      ...    ...       ...            ...     ...  \n",
       "970    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "971      ::ffff:54.243.49.1   POST  HTTP/1.1            200       -  \n",
       "972    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "973     ::ffff:69.47.237.44   POST  HTTP/1.1            200       -  \n",
       "974      ::ffff:54.243.49.1   POST  HTTP/1.1            200       -  \n",
       "975    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "976     ::ffff:86.103.139.9   POST  HTTP/1.1            200       -  \n",
       "977    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "978     ::ffff:86.103.139.9   POST  HTTP/1.1            200       -  \n",
       "979     ::ffff:86.103.139.9   POST  HTTP/1.1            200       -  \n",
       "980     ::ffff:86.103.139.9   POST  HTTP/1.1            200       -  \n",
       "981    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "982    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "983    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "984    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "985    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "986    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "987      ::ffff:54.243.49.1   POST  HTTP/1.1            200       -  \n",
       "988    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "989    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "990   ::ffff:62.240.166.118   POST  HTTP/1.1            200       -  \n",
       "991    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "992     ::ffff:84.85.189.28   POST  HTTP/1.1            200       -  \n",
       "993    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "994    ::ffff:54.221.205.80   POST  HTTP/1.1            200       -  \n",
       "995     ::ffff:86.103.139.9   POST  HTTP/1.1            200       -  \n",
       "996      ::ffff:54.243.49.1   POST  HTTP/1.1            200       -  \n",
       "997     ::ffff:86.103.139.9   POST  HTTP/1.1            200       -  \n",
       "998     ::ffff:86.103.139.9   POST  HTTP/1.1            200       -  \n",
       "999     ::ffff:86.103.139.9   POST  HTTP/1.1            200       -  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf = df.limit(1000).toPandas()\n",
    "\n",
    "# pdf is a Pandas dataframe. All pandas API can be used on it.\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `schema` of the dataframe was defined by our custom parser. We can check it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- content_size: long (nullable = true)\n",
      " |-- date_time: string (nullable = true)\n",
      " |-- endpoint: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- response_code: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring a data set, it is always interesting to get per column statistics\n",
    "as `count`, `min`, `max`, `average` and `standard deviation` values. This can be done using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+--------------------+--------------------+--------------------+------+--------+------------------+-------+\n",
      "|summary|client_id|      content_size|           date_time|            endpoint|          ip_address|method|protocol|     response_code|user_id|\n",
      "+-------+---------+------------------+--------------------+--------------------+--------------------+------+--------+------------------+-------+\n",
      "|  count|   336895|            336895|              336895|              336895|              336895|336895|  336895|            336895| 336895|\n",
      "|   mean|     null| 38.14462072752638|                null|                null|                null|  null|    null|200.03355348105492|   null|\n",
      "| stddev|     null|1670.5260288494235|                null|                null|                null|  null|    null| 2.602415337476263|   null|\n",
      "|    min|        -|                 2|01/Mar/2018:23:07...|                   /|::ffff:101.141.22.75|   GET|HTTP/1.1|               200|      -|\n",
      "|    max|        -|            685173|02/Mar/2018:21:34...|/v1/public/data/r...|::ffff:96.231.156...|  POST|HTTP/1.1|               404|      -|\n",
      "+-------+---------+------------------+--------------------+--------------------+--------------------+------+--------+------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>client_id</th>\n",
       "      <th>content_size</th>\n",
       "      <th>date_time</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>ip_address</th>\n",
       "      <th>method</th>\n",
       "      <th>protocol</th>\n",
       "      <th>response_code</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>336895</td>\n",
       "      <td>336895</td>\n",
       "      <td>336895</td>\n",
       "      <td>336895</td>\n",
       "      <td>336895</td>\n",
       "      <td>336895</td>\n",
       "      <td>336895</td>\n",
       "      <td>336895</td>\n",
       "      <td>336895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>38.14462072752638</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>200.03355348105492</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>1670.5260288494235</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.602415337476263</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "      <td>01/Mar/2018:23:07:31 +0000</td>\n",
       "      <td>/</td>\n",
       "      <td>::ffff:101.141.22.75</td>\n",
       "      <td>GET</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>-</td>\n",
       "      <td>685173</td>\n",
       "      <td>02/Mar/2018:21:34:45 +0000</td>\n",
       "      <td>/v1/public/data/read/jarooony/Azubibuero/tempe...</td>\n",
       "      <td>::ffff:96.231.156.111</td>\n",
       "      <td>POST</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>404</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary client_id        content_size                   date_time  \\\n",
       "0   count    336895              336895                      336895   \n",
       "1    mean      None   38.14462072752638                        None   \n",
       "2  stddev      None  1670.5260288494235                        None   \n",
       "3     min         -                   2  01/Mar/2018:23:07:31 +0000   \n",
       "4     max         -              685173  02/Mar/2018:21:34:45 +0000   \n",
       "\n",
       "                                            endpoint             ip_address  \\\n",
       "0                                             336895                 336895   \n",
       "1                                               None                   None   \n",
       "2                                               None                   None   \n",
       "3                                                  /   ::ffff:101.141.22.75   \n",
       "4  /v1/public/data/read/jarooony/Azubibuero/tempe...  ::ffff:96.231.156.111   \n",
       "\n",
       "   method  protocol       response_code user_id  \n",
       "0  336895    336895              336895  336895  \n",
       "1    None      None  200.03355348105492    None  \n",
       "2    None      None   2.602415337476263    None  \n",
       "3     GET  HTTP/1.1                 200       -  \n",
       "4    POST  HTTP/1.1                 404       -  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In plain Spark\n",
    "df.describe().show()\n",
    "\n",
    "# OR using toPandas conversion\n",
    "display(df.describe().toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up Weblog data\n",
    "\n",
    "### Removing empty columns\n",
    "\n",
    "Some of the values in Apache Log are optional, this is the case of `client_id`\n",
    "and `user_id`. Check if these 2 columns are reported in the logs, and remove the\n",
    " corresponding columns if this is not the case.\n",
    "\n",
    "> Hint you might want to check unique values of these columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(user_id='-', client_id='-')]\n"
     ]
    }
   ],
   "source": [
    "# check distinct user and client ids\n",
    "ids = df.select('user_id', 'client_id').distinct().collect()\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, both columns have just one unique value: `-`. Let's now remove\n",
    "the columns from the dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+------+--------+-------------+\n",
      "|content_size|           date_time|            endpoint|          ip_address|method|protocol|response_code|\n",
      "+------------+--------------------+--------------------+--------------------+------+--------+-------------+\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Ra...|  ::ffff:54.243.49.1|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Ra...|  ::ffff:54.243.49.1|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Aa...| ::ffff:84.85.189.28|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Kl...| ::ffff:86.103.139.9|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Kl...| ::ffff:86.103.139.9|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Sa...| ::ffff:84.85.189.28|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/bl...| ::ffff:87.174.32.47|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Ra...|  ::ffff:54.243.49.1|  POST|HTTP/1.1|          200|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|\n",
      "|         307|01/Mar/2018:23:07...|/v1/data/write/Ch...|::ffff:62.240.166...|  POST|HTTP/1.1|          200|\n",
      "+------------+--------------------+--------------------+--------------------+------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping user_id and client_id as they have no actual values, just the '-'.\n",
    "ndf = df.drop('user_id', 'client_id')\n",
    "\n",
    "ndf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting from string to time\n",
    "\n",
    "As you could see in the datframe Schema, `date_time` column is in String format.\n",
    "as `dd/MMM/yyyy:HH:mm:ss Z`. When manipulating dates or date time data, it is\n",
    "always better to have it in the corresponding type.\n",
    "\n",
    "Spark provides utility functions to work with dates and times. Let's create two columns:\n",
    "* `ts`: with `date_time` column converted into a timestamp (`DateType`)\n",
    "* `day`: with `date_time` column converted into `dd-MM-yyy` format.\n",
    "\n",
    "We will use the following function (assuming Spark version >= v2.2):\n",
    "* `to_timestamp`: casts a String formatted date time into a `DateType` object\n",
    "  using an optionally provided format. This function is new in version 2.2.\n",
    "* `date_format`: converts a `DateType` into a String in the specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+------+--------+-------------+-------------------+----------+\n",
      "|content_size|           date_time|            endpoint|          ip_address|method|protocol|response_code|                 ts|       day|\n",
      "+------------+--------------------+--------------------+--------------------+------+--------+-------------+-------------------+----------+\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Ra...|  ::ffff:54.243.49.1|  POST|HTTP/1.1|          200|2018-03-02 00:07:31|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Ra...|  ::ffff:54.243.49.1|  POST|HTTP/1.1|          200|2018-03-02 00:07:31|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:31|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Aa...| ::ffff:84.85.189.28|  POST|HTTP/1.1|          200|2018-03-02 00:07:31|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:32|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Kl...| ::ffff:86.103.139.9|  POST|HTTP/1.1|          200|2018-03-02 00:07:33|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Kl...| ::ffff:86.103.139.9|  POST|HTTP/1.1|          200|2018-03-02 00:07:33|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:33|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:34|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:34|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:34|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/bb...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:34|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Sa...| ::ffff:84.85.189.28|  POST|HTTP/1.1|          200|2018-03-02 00:07:34|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:34|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:35|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/bl...| ::ffff:87.174.32.47|  POST|HTTP/1.1|          200|2018-03-02 00:07:36|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:36|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/write/Ra...|  ::ffff:54.243.49.1|  POST|HTTP/1.1|          200|2018-03-02 00:07:36|02-03-2018|\n",
      "|           4|01/Mar/2018:23:07...|/v1/data/publish/...|::ffff:54.221.205.80|  POST|HTTP/1.1|          200|2018-03-02 00:07:37|02-03-2018|\n",
      "|         307|01/Mar/2018:23:07...|/v1/data/write/Ch...|::ffff:62.240.166...|  POST|HTTP/1.1|          200|2018-03-02 00:07:37|02-03-2018|\n",
      "+------------+--------------------+--------------------+--------------------+------+--------+-------------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "ndf = ndf.withColumn('ts', F.to_timestamp(ndf.date_time, 'dd/MMM/yyyy:HH:mm:ss Z'))\n",
    "ndf = ndf.withColumn('day', F.date_format(ndf.ts, 'dd-MM-yyyy'))\n",
    "\n",
    "ndf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Weblog data\n",
    "\n",
    "Let's now analyse the Weblog data to produce some reports. We will try to identify:\n",
    "* Top IP addresses: IP addresses with the largest number of queries. Remember a\n",
    "  Web query corresponds to a Weblog data entry. For every IP address, report the\n",
    "  queries count and the content size.\n",
    "* Top Endpoints: Endpoints with the largest number of requests. For every Endpoint,\n",
    "  report the queries count and the total content size.\n",
    "* Top Methods: number of queries and total content per Method.\n",
    "* Requests count per day: Number of requests per day.\n",
    "* Distribution of response codes.\n",
    "\n",
    "### Top IP addresses\n",
    "\n",
    "In order to get the top IP addresses with respect to queries count and content size,\n",
    "we need to `groupBy` IP addresse then `aggregate` by row `count` and by `sum` of\n",
    "content size. This can be done as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+\n",
      "|          ip_address|count|   size|\n",
      "+--------------------+-----+-------+\n",
      "|::ffff:54.221.205.80|93234| 372936|\n",
      "|::ffff:171.4.246.231|52924| 212611|\n",
      "|::ffff:114.182.22...|42035|4161435|\n",
      "|  ::ffff:54.243.49.1|29892| 119568|\n",
      "|::ffff:86.103.139.38|26022| 104088|\n",
      "|::ffff:190.192.57...|17026|  68104|\n",
      "| ::ffff:84.85.189.28| 4608|  18432|\n",
      "|::ffff:146.198.22...| 4444|  17776|\n",
      "|   ::ffff:2.30.7.202| 4303|  17212|\n",
      "| ::ffff:86.103.139.9| 3861|  15444|\n",
      "+--------------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "top_ips = ndf.groupBy('ip_address').agg(F.count(ndf.method).alias('count'), F.sum(ndf.content_size).alias('size'))\n",
    "\n",
    "top_ips.sort(F.desc('count')).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report we used the following:\n",
    "* [pyspark.sql.DataFrame.groupBy](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy)\n",
    "  to group the Weblog data using the `ip_address` column.\n",
    "* [pyspark.sql.GroupedData.agg](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg)\n",
    "  to aggregate grouped data according to the specified aggregation functions.\n",
    "* [pyspark.sql.function.count](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.count)\n",
    "  to return the number of items in a group.\n",
    "* [pyspark.sql.function.sum](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.sum)\n",
    "  to return the sum of all values, in our case the content size.\n",
    "* [pyspark.sql.Column.alias](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.Column.alias)\n",
    "  to rename a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Endpoints\n",
    "\n",
    "Similar to top IP addresses, we can get top endpoints as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            endpoint|count|\n",
      "+--------------------+-----+\n",
      "|/v1/data/publish/...|79870|\n",
      "|/v1/data/read/app...|42035|\n",
      "|/v1/data/write/Kl...|13646|\n",
      "|/v1/data/write/Kl...|13639|\n",
      "|/v1/data/write/Ra...|13501|\n",
      "|/v1/data/write/IS...| 7984|\n",
      "|/v1/data/write/Ra...| 7077|\n",
      "|/v1/data/write/Ra...| 7076|\n",
      "|/v1/data/write/Ra...| 7071|\n",
      "|/v1/data/write/Ra...| 4774|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "top_endpoints = ndf.groupBy('endpoint').agg(F.count(ndf.method).alias('count'))\n",
    "\n",
    "top_endpoints.sort(F.desc('count')).limit(10).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Methods\n",
    "\n",
    "Top methods can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|method| count|   size|\n",
      "+------+------+-------+\n",
      "|  POST|293601|6984063|\n",
      "|   GET| 43289|5866513|\n",
      "|  HEAD|     5|    156|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "top_methods = ndf.groupBy('method').agg(F.count(ndf.method).alias('count'), F.sum(ndf.content_size).alias('size'))\n",
    "\n",
    "top_methods.sort(F.desc('count')).limit(10).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries per day\n",
    "\n",
    "Number of queries per day can be calculated by grouping row count by `day` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|       day| count|\n",
      "+----------+------+\n",
      "|02-03-2018|336895|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "daily_requests = ndf.groupBy('day').agg(F.count(ndf.method).alias('count'))\n",
    "\n",
    "daily_requests.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response code distribution\n",
    "\n",
    "When analyzing Web server logs, it is always important to analyze response codes.\n",
    "The number or ratio of `4xx` and `5xx` response codes can be indicators of problems\n",
    "affecting the services. We can calculate the distribution of response codes as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|response_code| count|\n",
      "+-------------+------+\n",
      "|          404|    26|\n",
      "|          200|336839|\n",
      "|          400|    30|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "codes = ndf.groupBy('response_code').agg(F.count(ndf.method).alias('count'))\n",
    "\n",
    "codes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data to File System\n",
    "\n",
    "Spark defines an `interface`, `DataFrameWriter`, to write data to storage systems.\n",
    "Storage systems can be HDFS, local file system, key-value stores, etc.\n",
    "\n",
    "The `DataFrame.write()` method can be used to access the implemented storage systems.\n",
    "Out of the box, Spark can write local file system and HDFS.\n",
    "\n",
    "> Writing to Local File system: use this only on your single instance cluster or\n",
    "for development. Writing to Local File system is a very bad idea on a Spark cluster.\n",
    "You should rather use HDFS or alternative distributed and durable stores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing in CSV format\n",
    "\n",
    "You can write to CSV format as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .format('csv') \\\n",
    "  .save('PATH_TO_FOLDER/myweblog.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .csv('PATH_TO_FOLDER/myweblog.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run the write operation again! You should get an error `path already exists`.\n",
    "By default, the behaviour of Spark Writer is to raise an error if it tries\n",
    "writing to existing destination. You can override the default behavior by setting\n",
    "the mode option. You can select from:\n",
    "* `append`: appends content of the dataframe to write to existing data.\n",
    "* `overwrite`: Replaces existing data by the content of the dataframe to write.\n",
    "* `ignore`: Do nothing if the destination already exist.\n",
    "* `error`: this is the default mode. It throws an error if destination already exists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To override the destination with new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .csv('PATH_TO_FOLDER/myweblog.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The override mode is idempotent, writing multiple times will lead to the same\n",
    "data on the file system. This is not the case of `append` mode.\n",
    "Change your code to write with append mode, run it multiple times and see how\n",
    "data changes on disk.\n",
    "\n",
    "> Selecting the right write mode is use case specific. You should take good care\n",
    "when setting the write mode.\n",
    "\n",
    "Open the csv file (or any of them) in a text editor and see how it was saved without\n",
    "the column names. To include the header line containing the column names, you need\n",
    "to set the `Header` option:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .csv('PATH_TO_FOLDER/myweblog.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, writing to csv will not compress data. You can add compression by\n",
    "setting the `compression` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"compression\", \"gzip\") \\\n",
    "  .csv('PATH_TO_FOLDER/myweblog.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing in Parquet format\n",
    "\n",
    "Writing to parquet (or any other format) is almost identical to writing to csv\n",
    "format if we don't consider the csv specific serialization options. You can write\n",
    "your dataframe in Parquet format as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .format('parquet') \\\n",
    "  .save('PATH_TO_FOLDER/myweblog.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .parquet('PATH_TO_FOLDER/myweblog.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your local file system where data was written and check the size of the\n",
    "Parquet data. Compare it to CSV file size on disk to see how Parquet optimizes\n",
    "data thanks to its columnar format and optimization techniques.\n",
    "\n",
    "Now use `parquet-tools` to inspect the file and answer the following questions:\n",
    "* What is the size of every column on disk?\n",
    "* What type of statistics Parquet stores in columns metadata?\n",
    "* Does Parquet stores the data schema? if yes what is the schema? Compare it to\n",
    "  the schema of Spark dataframe.\n",
    "* Play with the compression mode to test `none`, `snappy` and `gzip`. What can\n",
    "  you say about the compression of Parquet data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing in ORC format\n",
    "\n",
    "Writing to ORC format is similar to writing to Parquet format. The only thing\n",
    "that changes if the format option:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .format('orc') \\\n",
    "  .save('PATH_TO_FOLDER/myweblog.orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .orc('PATH_TO_FOLDER/myweblog.orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `orc-tools` to inspect the file and answer the following questions:\n",
    "* What is the size of every column on disk?\n",
    "* What type of statistics ORC stores in columns metadata?\n",
    "* Does ORC stores the data schema? if yes what is the schema? Compare it to\n",
    "  the schema of Spark dataframe.\n",
    "* Play with the compression mode to test `none`, `snappy` and `gzip`. What can\n",
    "  you say about the compression of ORC data? How does it compare to Parquet?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing in Avro format\n",
    "\n",
    "Avro is a major binary row based format in the Hadoop ecosystem. However, Spark\n",
    "does not provide a native Avro connector. Avro connector is available through\n",
    "a third party package provided by Databricks. You need to explicitly import the\n",
    "Avro connector.\n",
    "\n",
    "Writing to Avro can be done as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .format('avro') \\\n",
    "  .save('PATH_TO_FOLDER/myweblog.avro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will notice, writing to avro will fail with the following message:\n",
    "\n",
    "> AnalysisException: 'Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;'\n",
    "\n",
    "In order to overcome this error, you need to explicitly load the Spark Avro package as described in this [guide](https://spark.apache.org/docs/2.4.1/sql-data-sources-avro.html).\n",
    "\n",
    "Stop your `pyspark` Jupyter Notebook integration and start it again with:\n",
    "\n",
    "```\n",
    "## For Scala v2.12\n",
    "pyspark --packages org.apache.spark:spark-avro_2.12:2.4.1\n",
    "\n",
    "## For Scala v2.11\n",
    "pyspark --packages org.apache.spark:spark-avro_2.11:2.4.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Data on File System\n",
    "\n",
    "Organizing stored data is a key point to consider when creating a data lake.\n",
    "How data is organized on file system affects the performance of both the reads\n",
    "and writes operations. It has also a direct impact on the volume of stored data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition by Columns\n",
    "\n",
    "Partitioning data by columns allows to physically divide data on file system\n",
    "according to the columns values. The advantage of partitioning by columns is a\n",
    "much faster data access speeds. The drawback is the danger of having too much\n",
    "files if the columns have a large number of unique values.\n",
    "\n",
    "Therefore, partitioning by columns should be based on the use case requirements.\n",
    "\n",
    "Let's see how partitioning by column works:\n",
    "\n",
    "We will first try to partition our data by `method` column. The objective is to\n",
    "have data files per `method`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.write \\\n",
    "  .partitionBy(\"method\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .parquet(\"PATH_TO_FOLDER/myweblog_partitioned.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal and check the data structure in your destination output. You\n",
    "should have something similar to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH_TO_FOLDER/myweblog_partitioned.parquet/:\n",
      "total 24\n",
      "drwxr-xr-x 5 ubuntu ubuntu 4096 avril 28 01:18 .\n",
      "drwxr-xr-x 8 ubuntu ubuntu 4096 avril 28 01:17 ..\n",
      "drwxr-xr-x 2 ubuntu ubuntu 4096 avril 28 01:18 method=GET\n",
      "drwxr-xr-x 2 ubuntu ubuntu 4096 avril 28 01:17 method=HEAD\n",
      "drwxr-xr-x 2 ubuntu ubuntu 4096 avril 28 01:18 method=POST\n",
      "-rw-r--r-- 1 ubuntu ubuntu    0 avril 28 01:18 _SUCCESS\n",
      "-rw-r--r-- 1 ubuntu ubuntu    8 avril 28 01:18 ._SUCCESS.crc\n",
      "\n",
      "PATH_TO_FOLDER/myweblog_partitioned.parquet/method=GET:\n",
      "total 356\n",
      "drwxr-xr-x 2 ubuntu ubuntu   4096 avril 28 01:18 .\n",
      "drwxr-xr-x 5 ubuntu ubuntu   4096 avril 28 01:18 ..\n",
      "-rw-r--r-- 1 ubuntu ubuntu  46960 avril 28 01:17 part-00000-ba45b85e-71d9-45cc-8cd6-20f06a9d6ad0.c000.snappy.parquet\n",
      "-rw-r--r-- 1 ubuntu ubuntu    376 avril 28 01:17 .part-00000-ba45b85e-71d9-45cc-8cd6-20f06a9d6ad0.c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 ubuntu ubuntu 296923 avril 28 01:18 part-00001-ba45b85e-71d9-45cc-8cd6-20f06a9d6ad0.c000.snappy.parquet\n",
      "-rw-r--r-- 1 ubuntu ubuntu   2328 avril 28 01:18 .part-00001-ba45b85e-71d9-45cc-8cd6-20f06a9d6ad0.c000.snappy.parquet.crc\n",
      "\n",
      "PATH_TO_FOLDER/myweblog_partitioned.parquet/method=HEAD:\n",
      "total 16\n",
      "drwxr-xr-x 2 ubuntu ubuntu 4096 avril 28 01:17 .\n",
      "drwxr-xr-x 5 ubuntu ubuntu 4096 avril 28 01:18 ..\n",
      "-rw-r--r-- 1 ubuntu ubuntu 2604 avril 28 01:17 part-00000-ba45b85e-71d9-45cc-8cd6-20f06a9d6ad0.c000.snappy.parquet\n",
      "-rw-r--r-- 1 ubuntu ubuntu   32 avril 28 01:17 .part-00000-ba45b85e-71d9-45cc-8cd6-20f06a9d6ad0.c000.snappy.parquet.crc\n",
      "\n",
      "PATH_TO_FOLDER/myweblog_partitioned.parquet/method=POST:\n",
      "total 2204\n",
      "drwxr-xr-x 2 ubuntu ubuntu    4096 avril 28 01:18 .\n",
      "drwxr-xr-x 5 ubuntu ubuntu    4096 avril 28 01:18 ..\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1717811 avril 28 01:18 part-00000-ba45b85e-71d9-45cc-8cd6-20f06a9d6ad0.c000.snappy.parquet\n",
      "-rw-r--r-- 1 ubuntu ubuntu   13432 avril 28 01:18 .part-00000-ba45b85e-71d9-45cc-8cd6-20f06a9d6ad0.c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 ubuntu ubuntu  505889 avril 28 01:18 part-00001-ba45b85e-71d9-45cc-8cd6-20f06a9d6ad0.c000.snappy.parquet\n",
      "-rw-r--r-- 1 ubuntu ubuntu    3964 avril 28 01:18 .part-00001-ba45b85e-71d9-45cc-8cd6-20f06a9d6ad0.c000.snappy.parquet.crc\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -all -R PATH_TO_FOLDER/myweblog_partitioned.parquet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the file system layout reflects perfectly the content of the `method`\n",
    "column. Now, any query with a condition on the method column will only read the\n",
    "corresponding partitions.\n",
    "\n",
    "Try now to partition by `ip_address`, check the destination folder and answer to\n",
    "the following questions:\n",
    "* Why in our example partitioning by IP address is a bad idea?\n",
    "* What columns in our data set you consider good choices to partition data on?\n",
    "\n",
    "It is also possible to partition by multiple columns. How would you partition\n",
    "data based on `method` and `response_code`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## partition by method and response_code\n",
    "## Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output destination on your file system.\n",
    "* Does the column order in `partitionBy` has any impact?\n",
    "* In the case of `method` and `response_code` columns, what would be the good order?\n",
    "* Assume you have the following columns: `year`, `month`, `day`, how would you\n",
    "  partition based on these columns? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Sorted data\n",
    "\n",
    "Saving sorted data to the file system can have double impact:\n",
    "* reduce the required disk space when using columnar based formats\n",
    "* improve the read performance of requests with conditions on the sorted columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df \\\n",
    "  .sort(F.desc('date_time')) \\\n",
    "  .repartition(1) \\\n",
    "  .write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .parquet(\"PATH_TO_FOLDER/myweblog_sorted.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare now the size on disk of sorted and non sorted parquet data.\n",
    "* What is the impact of sorting on `date_time` column on the file size?\n",
    "  Why in your opinion?\n",
    "\n",
    "Sorting based on *poorly selected* columns might have negative impact. Sort your\n",
    "data now based on `endpoint` column then write it to parquet.\n",
    "* What is the impact of sorting on `endpoint` column on the file size?\n",
    "  Why in your opinion?\n",
    "\n",
    "You can also sort data on multiple columns. Consider `endpoint`, `data_time` and\n",
    "`ip_address` columns. You are asked to find the best sorting strategy that minimizes\n",
    "disk space requirements.\n",
    "* Does sort order have an impact?\n",
    "* What is the sorting order that minimizes disk space? Why?\n",
    "\n",
    "> Hint: you can use `parquet-tools` to debug the output files.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
