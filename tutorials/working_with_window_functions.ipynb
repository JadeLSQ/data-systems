{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Window Functions in Spark\n",
    "\n",
    "In this tutorial, we will work with Window function in Spark. Window Functions\n",
    "were introduced in Spark v1.4 and are explained in this awesome Databricks\n",
    "[blog post](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html).\n",
    "\n",
    "For this tutorial, we will use the following sample data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = spark.createDataFrame([\n",
    "  (1234, '2018-03-01', 40),\n",
    "  (1234, '2018-03-15', 25),\n",
    "  (1234, '2018-04-01', 25),\n",
    "  (1234, '2018-04-30', 30),\n",
    "  (4567, '2018-03-01', 200),\n",
    "  (4567, '2018-03-13', 221),\n",
    "  (4567, '2018-04-01', 700),\n",
    "  (4567, '2018-04-06', 400),\n",
    "  (4567, '2018-04-29', 500),\n",
    "  (9999, '2018-03-01', 150),\n",
    "  (8888, '2018-03-01', 15),\n",
    "  (8888, '2018-03-30', 20)\n",
    "]).toDF('id', 'start', 'nb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with SQL, let's create a temporary view over our dataframe. A view\n",
    "can be used exactly as a table (i.e. Hive Table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf.createOrReplaceTempView('wdftable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will create windows over the dataframe partitioned by `id`\n",
    "column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Window\n",
    "\n",
    "A Window over a dataframe can be defined as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "## create a window partitioned by id and order windows data by start column\n",
    "win = Window.partitionBy('id').orderBy(wdf.start.desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Operations\n",
    "\n",
    "In the following we will review a large set of window functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank\n",
    "\n",
    "`pyspark.sql.functions.rank()` is a window function that returns the rank of\n",
    "rows within a window partition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+----+\n",
      "|  id|     start| nb|rank|\n",
      "+----+----------+---+----+\n",
      "|8888|2018-03-30| 20|   1|\n",
      "|8888|2018-03-01| 15|   2|\n",
      "|9999|2018-03-01|150|   1|\n",
      "|1234|2018-04-30| 30|   1|\n",
      "|1234|2018-04-01| 25|   2|\n",
      "|1234|2018-03-15| 25|   3|\n",
      "|1234|2018-03-01| 40|   4|\n",
      "|4567|2018-04-29|500|   1|\n",
      "|4567|2018-04-06|400|   2|\n",
      "|4567|2018-04-01|700|   3|\n",
      "|4567|2018-03-13|221|   4|\n",
      "|4567|2018-03-01|200|   5|\n",
      "+----+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "rankfct = F.rank().over(win)\n",
    "rankdf = wdf.select('*', rankfct.alias('rank'))\n",
    "\n",
    "rankdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `SQL`, the same operation can be achieved with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+----+\n",
      "|  id|     start| nb|rank|\n",
      "+----+----------+---+----+\n",
      "|8888|2018-03-30| 20|   1|\n",
      "|8888|2018-03-01| 15|   2|\n",
      "|9999|2018-03-01|150|   1|\n",
      "|1234|2018-04-30| 30|   1|\n",
      "|1234|2018-04-01| 25|   2|\n",
      "|1234|2018-03-15| 25|   3|\n",
      "|1234|2018-03-01| 40|   4|\n",
      "|4567|2018-04-29|500|   1|\n",
      "|4567|2018-04-06|400|   2|\n",
      "|4567|2018-04-01|700|   3|\n",
      "|4567|2018-03-13|221|   4|\n",
      "|4567|2018-03-01|200|   5|\n",
      "+----+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rankdf = spark.sql(\"SELECT id, start, nb, RANK() OVER (PARTITION BY id ORDER BY start desc) as rank FROM wdftable\")\n",
    "\n",
    "rankdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum\n",
    "\n",
    "When using the `sum` function over a window, we will have the cumulative `sum`\n",
    "as we move over the Window. Check the results given by this sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+----+\n",
      "|  id|     start| nb| sum|\n",
      "+----+----------+---+----+\n",
      "|8888|2018-03-30| 20|  20|\n",
      "|8888|2018-03-01| 15|  35|\n",
      "|9999|2018-03-01|150| 150|\n",
      "|1234|2018-04-30| 30|  30|\n",
      "|1234|2018-04-01| 25|  55|\n",
      "|1234|2018-03-15| 25|  80|\n",
      "|1234|2018-03-01| 40| 120|\n",
      "|4567|2018-04-29|500| 500|\n",
      "|4567|2018-04-06|400| 900|\n",
      "|4567|2018-04-01|700|1600|\n",
      "|4567|2018-03-13|221|1821|\n",
      "|4567|2018-03-01|200|2021|\n",
      "+----+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cumulsum = F.sum(wdf.nb).over(win)\n",
    "\n",
    "sumdf = wdf.select('*', cumulsum.alias('sum'))\n",
    "\n",
    "sumdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `SQL`, the same operation can be achieved with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+----+\n",
      "|  id|     start| nb| sum|\n",
      "+----+----------+---+----+\n",
      "|8888|2018-03-30| 20|  20|\n",
      "|8888|2018-03-01| 15|  35|\n",
      "|9999|2018-03-01|150| 150|\n",
      "|1234|2018-04-30| 30|  30|\n",
      "|1234|2018-04-01| 25|  55|\n",
      "|1234|2018-03-15| 25|  80|\n",
      "|1234|2018-03-01| 40| 120|\n",
      "|4567|2018-04-29|500| 500|\n",
      "|4567|2018-04-06|400| 900|\n",
      "|4567|2018-04-01|700|1600|\n",
      "|4567|2018-03-13|221|1821|\n",
      "|4567|2018-03-01|200|2021|\n",
      "+----+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sumdf = spark.sql(\"SELECT id, start, nb, sum(nb) OVER (PARTITION BY id ORDER BY start desc) as sum FROM wdftable\")\n",
    "\n",
    "sumdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lead\n",
    "\n",
    "`lead` is a window function that returns the value that is one (by default) row\n",
    "after the current row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+--------------+\n",
      "|  id|     start| nb|previous_start|\n",
      "+----+----------+---+--------------+\n",
      "|8888|2018-03-30| 20|    2018-03-01|\n",
      "|8888|2018-03-01| 15|          null|\n",
      "|9999|2018-03-01|150|          null|\n",
      "|1234|2018-04-30| 30|    2018-04-01|\n",
      "|1234|2018-04-01| 25|    2018-03-15|\n",
      "|1234|2018-03-15| 25|    2018-03-01|\n",
      "|1234|2018-03-01| 40|          null|\n",
      "|4567|2018-04-29|500|    2018-04-06|\n",
      "|4567|2018-04-06|400|    2018-04-01|\n",
      "|4567|2018-04-01|700|    2018-03-13|\n",
      "|4567|2018-03-13|221|    2018-03-01|\n",
      "|4567|2018-03-01|200|          null|\n",
      "+----+----------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leadfct = F.lead(wdf.start, 1).over(win)\n",
    "leaddf = wdf.select('*', leadfct.alias('previous_start'))\n",
    "\n",
    "leaddf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `SQL`, the same operation can be achieved with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+--------------+\n",
      "|  id|     start| nb|previous_start|\n",
      "+----+----------+---+--------------+\n",
      "|8888|2018-03-30| 20|    2018-03-01|\n",
      "|8888|2018-03-01| 15|          null|\n",
      "|9999|2018-03-01|150|          null|\n",
      "|1234|2018-04-30| 30|    2018-04-01|\n",
      "|1234|2018-04-01| 25|    2018-03-15|\n",
      "|1234|2018-03-15| 25|    2018-03-01|\n",
      "|1234|2018-03-01| 40|          null|\n",
      "|4567|2018-04-29|500|    2018-04-06|\n",
      "|4567|2018-04-06|400|    2018-04-01|\n",
      "|4567|2018-04-01|700|    2018-03-13|\n",
      "|4567|2018-03-13|221|    2018-03-01|\n",
      "|4567|2018-03-01|200|          null|\n",
      "+----+----------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leaddf = spark.sql('SELECT id, start, nb, LEAD(start) OVER (PARTITION BY id ORDER BY start DESC) as previous_start from wdftable')\n",
    "\n",
    "leaddf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag\n",
    "\n",
    "`lag` is a window function that returns the value that is one (by default) row\n",
    "before the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+----------+\n",
      "|  id|     start| nb|       end|\n",
      "+----+----------+---+----------+\n",
      "|8888|2018-03-30| 20|      null|\n",
      "|8888|2018-03-01| 15|2018-03-30|\n",
      "|9999|2018-03-01|150|      null|\n",
      "|1234|2018-04-30| 30|      null|\n",
      "|1234|2018-04-01| 25|2018-04-30|\n",
      "|1234|2018-03-15| 25|2018-04-01|\n",
      "|1234|2018-03-01| 40|2018-03-15|\n",
      "|4567|2018-04-29|500|      null|\n",
      "|4567|2018-04-06|400|2018-04-29|\n",
      "|4567|2018-04-01|700|2018-04-06|\n",
      "|4567|2018-03-13|221|2018-04-01|\n",
      "|4567|2018-03-01|200|2018-03-13|\n",
      "+----+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagfct = F.lag(wdf.start, 1).over(win)\n",
    "lagdf = wdf.select('*', lagfct.alias('end'))\n",
    "\n",
    "lagdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `SQL`, the same operation can be achieved with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+----------+\n",
      "|  id|     start| nb|       end|\n",
      "+----+----------+---+----------+\n",
      "|8888|2018-03-30| 20|      null|\n",
      "|8888|2018-03-01| 15|2018-03-30|\n",
      "|9999|2018-03-01|150|      null|\n",
      "|1234|2018-04-30| 30|      null|\n",
      "|1234|2018-04-01| 25|2018-04-30|\n",
      "|1234|2018-03-15| 25|2018-04-01|\n",
      "|1234|2018-03-01| 40|2018-03-15|\n",
      "|4567|2018-04-29|500|      null|\n",
      "|4567|2018-04-06|400|2018-04-29|\n",
      "|4567|2018-04-01|700|2018-04-06|\n",
      "|4567|2018-03-13|221|2018-04-01|\n",
      "|4567|2018-03-01|200|2018-03-13|\n",
      "+----+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lagdf = spark.sql('SELECT id, start, nb, LAG(start) OVER (PARTITION BY id ORDER BY start DESC) as end from wdftable')\n",
    "\n",
    "lagdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First\n",
    "\n",
    "`first` returns the first value in a window partition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+----------+\n",
      "|  id|     start| nb|     first|\n",
      "+----+----------+---+----------+\n",
      "|8888|2018-03-30| 20|2018-03-30|\n",
      "|8888|2018-03-01| 15|2018-03-30|\n",
      "|9999|2018-03-01|150|2018-03-01|\n",
      "|1234|2018-04-30| 30|2018-04-30|\n",
      "|1234|2018-04-01| 25|2018-04-30|\n",
      "|1234|2018-03-15| 25|2018-04-30|\n",
      "|1234|2018-03-01| 40|2018-04-30|\n",
      "|4567|2018-04-29|500|2018-04-29|\n",
      "|4567|2018-04-06|400|2018-04-29|\n",
      "|4567|2018-04-01|700|2018-04-29|\n",
      "|4567|2018-03-13|221|2018-04-29|\n",
      "|4567|2018-03-01|200|2018-04-29|\n",
      "+----+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstfct = F.first(wdf.start).over(win)\n",
    "firstdf = wdf.select('*', firstfct.alias('first'))\n",
    "\n",
    "firstdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `SQL`, the same operation can be achieved with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+----------+\n",
      "|  id|     start| nb|     first|\n",
      "+----+----------+---+----------+\n",
      "|8888|2018-03-30| 20|2018-03-30|\n",
      "|8888|2018-03-01| 15|2018-03-30|\n",
      "|9999|2018-03-01|150|2018-03-01|\n",
      "|1234|2018-04-30| 30|2018-04-30|\n",
      "|1234|2018-04-01| 25|2018-04-30|\n",
      "|1234|2018-03-15| 25|2018-04-30|\n",
      "|1234|2018-03-01| 40|2018-04-30|\n",
      "|4567|2018-04-29|500|2018-04-29|\n",
      "|4567|2018-04-06|400|2018-04-29|\n",
      "|4567|2018-04-01|700|2018-04-29|\n",
      "|4567|2018-03-13|221|2018-04-29|\n",
      "|4567|2018-03-01|200|2018-04-29|\n",
      "+----+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstdf = spark.sql('SELECT id, start, nb, FIRST(start) OVER (PARTITION BY id ORDER BY start DESC) as first from wdftable')\n",
    "\n",
    "firstdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last\n",
    "\n",
    "`last` returns the last value in a window partition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+---------------+\n",
      "|  id|     start| nb|last_not_really|\n",
      "+----+----------+---+---------------+\n",
      "|8888|2018-03-30| 20|     2018-03-30|\n",
      "|8888|2018-03-01| 15|     2018-03-01|\n",
      "|9999|2018-03-01|150|     2018-03-01|\n",
      "|1234|2018-04-30| 30|     2018-04-30|\n",
      "|1234|2018-04-01| 25|     2018-04-01|\n",
      "|1234|2018-03-15| 25|     2018-03-15|\n",
      "|1234|2018-03-01| 40|     2018-03-01|\n",
      "|4567|2018-04-29|500|     2018-04-29|\n",
      "|4567|2018-04-06|400|     2018-04-06|\n",
      "|4567|2018-04-01|700|     2018-04-01|\n",
      "|4567|2018-03-13|221|     2018-03-13|\n",
      "|4567|2018-03-01|200|     2018-03-01|\n",
      "+----+----------+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lastfct = F.last(wdf.start).over(win)\n",
    "lastdf = wdf.select('*', lastfct.alias('last_not_really'))\n",
    "\n",
    "lastdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `SQL`, the same operation can be achieved with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+---------------+\n",
      "|  id|     start| nb|last_not_really|\n",
      "+----+----------+---+---------------+\n",
      "|8888|2018-03-30| 20|     2018-03-30|\n",
      "|8888|2018-03-01| 15|     2018-03-01|\n",
      "|9999|2018-03-01|150|     2018-03-01|\n",
      "|1234|2018-04-30| 30|     2018-04-30|\n",
      "|1234|2018-04-01| 25|     2018-04-01|\n",
      "|1234|2018-03-15| 25|     2018-03-15|\n",
      "|1234|2018-03-01| 40|     2018-03-01|\n",
      "|4567|2018-04-29|500|     2018-04-29|\n",
      "|4567|2018-04-06|400|     2018-04-06|\n",
      "|4567|2018-04-01|700|     2018-04-01|\n",
      "|4567|2018-03-13|221|     2018-03-13|\n",
      "|4567|2018-03-01|200|     2018-03-01|\n",
      "+----+----------+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lastdf = spark.sql('SELECT id, start, nb, LAST(start) OVER (PARTITION BY id ORDER BY start DESC) as last_not_really from wdftable')\n",
    "\n",
    "lastdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the results, the newly created column by `last` window function\n",
    "do not contain what we really expected. It contains exactly the same values as\n",
    "the content of `start` column. This is however normal, the reason is `range` of\n",
    "the Window being by default from [`Window.unboundedPreceding`, `Window.currentRow`]\n",
    "which practically means from all preceding row to the current row. Using such range\n",
    "it is not surprising that the `last` row will always be the current row.\n",
    "\n",
    "In order to consider the complete range of the window partitions, we need to\n",
    "specify the range:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+----------+\n",
      "|  id|     start| nb|      last|\n",
      "+----+----------+---+----------+\n",
      "|8888|2018-03-30| 20|2018-03-01|\n",
      "|8888|2018-03-01| 15|2018-03-01|\n",
      "|9999|2018-03-01|150|2018-03-01|\n",
      "|1234|2018-04-30| 30|2018-03-01|\n",
      "|1234|2018-04-01| 25|2018-03-01|\n",
      "|1234|2018-03-15| 25|2018-03-01|\n",
      "|1234|2018-03-01| 40|2018-03-01|\n",
      "|4567|2018-04-29|500|2018-03-01|\n",
      "|4567|2018-04-06|400|2018-03-01|\n",
      "|4567|2018-04-01|700|2018-03-01|\n",
      "|4567|2018-03-13|221|2018-03-01|\n",
      "|4567|2018-03-01|200|2018-03-01|\n",
      "+----+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## range from all preceding rows to all following rows\n",
    "win = Window.partitionBy('id').orderBy(wdf.start.desc()).rowsBetween(-Window.unboundedFollowing, Window.unboundedFollowing)\n",
    "\n",
    "lastfct = F.last(wdf.start).over(win)\n",
    "\n",
    "lastdf = wdf.select('*', lastfct.alias('last'))\n",
    "\n",
    "lastdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `SQL`, the same operation can be achieved with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---+----------+\n",
      "|  id|     start| nb|      last|\n",
      "+----+----------+---+----------+\n",
      "|8888|2018-03-30| 20|2018-03-01|\n",
      "|8888|2018-03-01| 15|2018-03-01|\n",
      "|9999|2018-03-01|150|2018-03-01|\n",
      "|1234|2018-04-30| 30|2018-03-01|\n",
      "|1234|2018-04-01| 25|2018-03-01|\n",
      "|1234|2018-03-15| 25|2018-03-01|\n",
      "|1234|2018-03-01| 40|2018-03-01|\n",
      "|4567|2018-04-29|500|2018-03-01|\n",
      "|4567|2018-04-06|400|2018-03-01|\n",
      "|4567|2018-04-01|700|2018-03-01|\n",
      "|4567|2018-03-13|221|2018-03-01|\n",
      "|4567|2018-03-01|200|2018-03-01|\n",
      "+----+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lastdf = spark.sql('SELECT id, start, nb, LAST(start) OVER (PARTITION BY id ORDER BY start DESC ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) as last from wdftable')\n",
    "\n",
    "lastdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is critical to correctly specify the range when defining a Window. The\n",
    "results highly depend on this range as we have seen with `last` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max\n",
    "\n",
    "`max` returns the maximum value in the window partition range.\n",
    "\n",
    "Using the comments from last section, add a new column to the dataframe containing\n",
    "the maximum value of `nb` column in the window partition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another column containing the difference between the maximum value over the\n",
    "window and the current value of the row. See how the window function can effectively\n",
    "be an expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## max - current value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
